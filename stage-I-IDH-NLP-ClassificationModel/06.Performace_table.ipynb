{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model [Thresholds=0.5]</th>\n",
       "      <th>Accruacy</th>\n",
       "      <th>Precision [macro]</th>\n",
       "      <th>Recall [macro]</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>F1-score [macro]</th>\n",
       "      <th>TP / TN</th>\n",
       "      <th>FP / FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mSBERT</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.930</td>\n",
       "      <td>104 / 368</td>\n",
       "      <td>22 / 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mUSE</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.932</td>\n",
       "      <td>103 / 370</td>\n",
       "      <td>20 / 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mSBERT-F</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.923</td>\n",
       "      <td>101 / 369</td>\n",
       "      <td>21 / 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mUSE-F</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.892</td>\n",
       "      <td>99 / 359</td>\n",
       "      <td>31 / 8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model [Thresholds=0.5]  Accruacy  Precision [macro]  Recall [macro]  \\\n",
       "0                 mSBERT     0.950              0.909           0.958   \n",
       "1                   mUSE     0.952              0.913           0.956   \n",
       "2               mSBERT-F     0.946              0.906           0.945   \n",
       "3                 mUSE-F     0.922              0.870           0.923   \n",
       "\n",
       "   Specificity  F1-score [macro]    TP / TN FP / FN  \n",
       "0        0.944             0.930  104 / 368  22 / 3  \n",
       "1        0.949             0.932  103 / 370  20 / 4  \n",
       "2        0.946             0.923  101 / 369  21 / 6  \n",
       "3        0.921             0.892   99 / 359  31 / 8  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calc fulltext and sentence performace\n",
    "model_name_s = \"ST-DBMCv1\" #ST-DBMCv1, mUSE\n",
    "ths = 0.5\n",
    "fulltext_mUSE = pd.read_csv(f'./other_experiment/ACM_mUSE_performace_table_predict_test_sample500_intra+peri-ths{ths}_fulltext.csv')\n",
    "fulltext_mSBERT = pd.read_csv(f'./other_experiment/ACM_ST-DBMCv1_performace_table_predict_test_sample500_intra+peri-ths{ths}_fulltext.csv')\n",
    "sentence_mUSE = pd.read_csv(f'./ACM_mUSE_performace_table_predict_test_sample500_intra+peri-ths{ths}.csv')\n",
    "sentence_mSBERT = pd.read_csv(f'./ACM_ST-DBMCv1_performace_table_predict_test_sample500_intra+peri-ths{ths}.csv')\n",
    "new_dict = {'mSBERT':sentence_mSBERT['IDH Judge'], 'mUSE': sentence_mUSE['IDH Judge'],\n",
    "            'mSBERT-F':fulltext_mSBERT['IDH Judge'], 'mUSE-F': fulltext_mUSE['IDH Judge'],}\n",
    "performance_fulltext_vs_sentence = (pd.DataFrame(new_dict).transpose())\n",
    "performance_fulltext_vs_sentence[[0,1,2,3,4]] = performance_fulltext_vs_sentence[[0,1,2,3,4]].astype(float).round(3)\n",
    "performance_fulltext_vs_sentence = performance_fulltext_vs_sentence.rename(columns={0:'Accruacy',1:'Precision [macro]',2:'Recall [macro]',\n",
    "                                                                                    3:'Specificity',4:'F1-score [macro]',5:'TP / TN', 6:'FP / FN'})\n",
    "performance_fulltext_vs_sentence.round(3).reset_index().rename(columns={'index':'Model [Thresholds=0.5]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yd_nlptf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
